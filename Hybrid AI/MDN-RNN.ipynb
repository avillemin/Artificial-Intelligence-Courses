{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the MDN-RNN model with TensorFlow\n",
    " \n",
    "# Importing the libraries\n",
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "# Building the MDN-RNN model within a class\n",
    " \n",
    "class MDNRNN(object):\n",
    "    \n",
    "    # Initializing all the parameters and variables of the MDNRNN class\n",
    "    def __init__(self, hps, reuse=False, gpu_mode=False):\n",
    "        self.hps = hps\n",
    "        with tf.variable_scope('mdn_rnn', reuse=reuse):\n",
    "            if not gpu_mode:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    tf.logging.info('Model using cpu.')\n",
    "                    self.g = tf.Graph()\n",
    "                    with self.g.as_default():\n",
    "                        self.build_model(hps)\n",
    "            else:\n",
    "                tf.logging.info('Model using gpu.')\n",
    "                self.g = tf.Graph()\n",
    "                with self.g.as_default():\n",
    "                    self.build_model(hps)\n",
    "        self._init_session()\n",
    "    \n",
    "    # Making a method that creates the MDN-RNN model architecture itself\n",
    "    def build_model(self, hps):\n",
    "        # Building the RNN\n",
    "        self.num_mixture = hps.num_mixture\n",
    "        KMIX = self.num_mixture\n",
    "        INWIDTH = hps.input_seq_width\n",
    "        OUTWIDTH = hps.output_seq_width\n",
    "        LENGTH = self.hps.max_seq_len\n",
    "        if hps.is_training:\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        cell_fn = tf.contrib.rnn.LayerNormBasicLSTMCell\n",
    "        use_recurrent_dropout = False if self.hps.use_recurrent_dropout == 0 else True\n",
    "        use_input_dropout = False if self.hps.use_input_dropout == 0 else True\n",
    "        use_output_dropout = False if self.hps.use_output_dropout == 0 else True\n",
    "        use_layer_norm = False if self.hps.use_layer_norm == 0 else True\n",
    "        if use_recurrent_dropout:\n",
    "            cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm, dropout_keep_prob=self.hps.recurrent_dropout_prob)\n",
    "        else:\n",
    "            cell = cell_fn(hps.rnn_size, layer_norm=use_layer_norm)\n",
    "        if use_input_dropout:\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.hps.input_dropout_prob)\n",
    "        if use_output_dropout:\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.hps.output_dropout_prob)\n",
    "        self.cell = cell\n",
    "        self.sequence_lengths = LENGTH\n",
    "        self.input_x = tf.placeholder(dtype=tf.float32, shape=[self.hps.batch_size, self.hps.max_seq_len, INWIDTH])\n",
    "        self.output_x = tf.placeholder(dtype=tf.float32, shape=[self.hps.batch_size, self.hps.max_seq_len, OUTWIDTH])\n",
    "        actual_input_x = self.input_x\n",
    "        self.initial_state = cell.zero_state(batch_size=hps.batch_size, dtype=tf.float32)\n",
    "        NOUT = OUTWIDTH * KMIX * 3\n",
    "        with tf.variable_scope('RNN'):\n",
    "            output_w = tf.get_variable(\"output_w\", [self.hps.rnn_size, NOUT])\n",
    "            output_b = tf.get_variable(\"output_b\", [NOUT])\n",
    "        output, last_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                               inputs=actual_input_x,\n",
    "                                               initial_state=self.initial_state,\n",
    "                                               dtype=tf.float32,\n",
    "                                               swap_memory=True,\n",
    "                                               scope=\"RNN\")\n",
    "        # Building the MDN\n",
    "        output = tf.reshape(output, [-1, hps.rnn_size])\n",
    "        output = tf.nn.xw_plus_b(output, output_w, output_b)\n",
    "        output = tf.reshape(output, [-1, KMIX * 3])\n",
    "        self.final_state = last_state\n",
    "        def get_mdn_coef(output):\n",
    "            logmix, mean, logstd = tf.split(output, 3, 1)\n",
    "            logmix = logmix - tf.reduce_logsumexp(logmix, 1, keepdims=True)\n",
    "            return logmix, mean, logstd\n",
    "        out_logmix, out_mean, out_logstd = get_mdn_coef(output)\n",
    "        self.out_logmix = out_logmix\n",
    "        self.out_mean = out_mean\n",
    "        self.out_logstd = out_logstd\n",
    "        # Implementing the training operations\n",
    "        logSqrtTwoPI = np.log(np.sqrt(2.0 * np.pi))\n",
    "        def tf_lognormal(y, mean, logstd):\n",
    "            return -0.5 * ((y - mean) / tf.exp(logstd)) ** 2 - logstd - logSqrtTwoPI\n",
    "        def get_lossfunc(logmix, mean, logstd, y):\n",
    "            v = logmix + tf_lognormal(y, mean, logstd)\n",
    "            v = tf.reduce_logsumexp(v, 1, keepdims=True)\n",
    "            return -tf.reduce_mean(v)\n",
    "        flat_target_data = tf.reshape(self.output_x,[-1, 1])\n",
    "        lossfunc = get_lossfunc(out_logmix, out_mean, out_logstd, flat_target_data)\n",
    "        self.cost = tf.reduce_mean(lossfunc)\n",
    "        if self.hps.is_training == 1:\n",
    "            self.lr = tf.Variable(self.hps.learning_rate, trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            gvs = self.optimizer.compute_gradients(self.cost)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -self.hps.grad_clip, self.hps.grad_clip), var) for grad, var in gvs]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_gvs, global_step=self.global_step, name='train_step')\n",
    "        self.init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Building the MDN-RNN model with Keras\n",
    " \n",
    "# Importing the libraries\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    " \n",
    "# Setting the dimensions of the latent vectors\n",
    "Z_DIM = 32\n",
    " \n",
    "# Setting the number of actions\n",
    "ACTION_DIM = 3\n",
    " \n",
    "# Setting the number of LSTM units\n",
    "HIDDEN_UNITS = 256\n",
    " \n",
    "# Setting the number of gaussian mixture outputs\n",
    "GAUSSIAN_MIXTURES = 5\n",
    " \n",
    "# Setting the batch size and number of epochs\n",
    "BATCH_SIZE =32\n",
    "EPOCHS = 20\n",
    " \n",
    "# Getting the gaussian mixture coeficients\n",
    "def get_mixture_coef(y_pred):\n",
    "    d = GAUSSIAN_MIXTURES * Z_DIM\n",
    "    rollout_length = K.shape(y_pred)[1]\n",
    "    pi = y_pred[:,:,:d]\n",
    "    mu = y_pred[:,:,d:(2*d)]\n",
    "    log_sigma = y_pred[:,:,(2*d):(3*d)]\n",
    "    pi = K.reshape(pi, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
    "    mu = K.reshape(mu, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
    "    log_sigma = K.reshape(log_sigma, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
    "    pi = K.exp(pi) / K.sum(K.exp(pi), axis=2, keepdims=True)\n",
    "    sigma = K.exp(log_sigma)\n",
    "    return pi, mu, sigma\n",
    " \n",
    "# Normalizing the target values\n",
    "def tf_normal(y_true, mu, sigma, pi):\n",
    "    rollout_length = K.shape(y_true)[1]\n",
    "    y_true = K.tile(y_true,(1,1,GAUSSIAN_MIXTURES))\n",
    "    y_true = K.reshape(y_true, [-1, rollout_length, GAUSSIAN_MIXTURES,Z_DIM])\n",
    "    oneDivSqrtTwoPI = 1 / math.sqrt(2*math.pi)\n",
    "    result = y_true - mu\n",
    "    result = result * (1 / (sigma + 1e-8))\n",
    "    result = -K.square(result)/2\n",
    "    result = K.exp(result) * (1/(sigma + 1e-8))*oneDivSqrtTwoPI\n",
    "    result = result * pi\n",
    "    result = K.sum(result, axis=2)\n",
    "    return result\n",
    " \n",
    "# Building the MDN-RNN model within a class\n",
    " \n",
    "class MDNRNN():\n",
    " \n",
    "    # Initializing all the parameters and variables of the MDNRNN class\n",
    "    def __init__(self):\n",
    "        self.models = self._build()\n",
    "        self.model = self.models[0]\n",
    "        self.forward = self.models[1]\n",
    "        self.z_dim = Z_DIM\n",
    "        self.action_dim = ACTION_DIM\n",
    "        self.hidden_units = HIDDEN_UNITS\n",
    "        self.gaussian_mixtures = GAUSSIAN_MIXTURES\n",
    " \n",
    "    # Building the model\n",
    "    def _build(self):\n",
    "        # Defining the Inputs of the RNN (latent vector space + action space)\n",
    "        rnn_x = Input(shape=(None, Z_DIM + ACTION_DIM))\n",
    "        # Defining the LSTM layer that returns the output weights and cell states\n",
    "        lstm = LSTM(HIDDEN_UNITS, return_sequences=True, return_state = True)\n",
    "        # Getting the real outputs from the LSTM\n",
    "        lstm_output, _ , _ = lstm(rnn_x)\n",
    "        # Getting the gaussian mixture outputs\n",
    "        mdn = Dense(GAUSSIAN_MIXTURES * (3*Z_DIM))(lstm_output)\n",
    "        # Getting the training model\n",
    "        rnn = Model(rnn_x, mdn)\n",
    "        # Getting the hidden state and cell state inputs\n",
    "        state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
    "        state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
    "        # Grouping them\n",
    "        state_inputs = [state_input_h, state_input_c]\n",
    "        # Getting outputs new state and new cell state from the LSTM\n",
    "        _ , state_h, state_c = lstm(rnn_x, initial_state = [state_input_h, state_input_c])\n",
    "        # Defining the forward propagation for inference only\n",
    "        forward = Model([rnn_x] + state_inputs, [state_h, state_c])\n",
    "        # Implementing the training operations\n",
    "        def rnn_r_loss(y_true, y_pred):\n",
    "            # Defining the negative log loss over all the gausian mixtures\n",
    "            pi, mu, sigma = get_mixture_coef(y_pred)\n",
    "            result = tf_normal(y_true, mu, sigma, pi)\n",
    "            result = -K.log(result + 1e-8)\n",
    "            result = K.mean(result, axis = (1,2))\n",
    "            return result\n",
    "        # Defining the KL divergence loss, the same as in the VAE, only over normalized outputs\n",
    "        def rnn_kl_loss(y_true, y_pred):\n",
    "            pi, mu, sigma = get_mixture_coef(y_pred)\n",
    "            kl_loss = - 0.5 * K.mean(1 + K.log(K.square(sigma)) - K.square(mu) - K.square(sigma), axis = [1,2,3])\n",
    "            return kl_loss\n",
    "        # Defining the RNN loss\n",
    "        def rnn_loss(y_true, y_pred):\n",
    "            return rnn_r_loss(y_true, y_pred)\n",
    "        # Compiling the RNN model with the RNN loss and the RMSProp optimizer\n",
    "        rnn.compile(loss=rnn_loss, optimizer='rmsprop', metrics = [rnn_r_loss, rnn_kl_loss])\n",
    "        return (rnn,forward)\n",
    " \n",
    "    # Loading the weights of the model\n",
    "    def set_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    " \n",
    "    # Creating early stopping callbacks to prevent overfitting\n",
    "    def train(self, rnn_input, rnn_output, validation_split = 0.2):\n",
    "        earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "        callbacks_list = [earlystop]\n",
    "        # Fitting the model to the RNN inputs and targets\n",
    "        self.model.fit(rnn_input, rnn_output,\n",
    "            shuffle=True,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=callbacks_list)\n",
    "        # Saving the model after the training is done\n",
    "        self.model.save_weights('rnn/weights.h5')\n",
    " \n",
    "    # Separating the function used to save the model (usefull if the model is retrained)\n",
    "    def save_weights(self, filepath):\n",
    "        self.model.save_weights(filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
