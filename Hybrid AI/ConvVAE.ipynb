{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN-VAE model with Tensorflow\n",
    " \n",
    "# Importing the libraries\n",
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "# Building the CNN-VAE model within a class\n",
    " \n",
    "class ConvVAE(object):\n",
    "    \n",
    "    # Initializing all the parameters and variables of the ConvVAE class\n",
    "    def __init__(self, z_size=32, batch_size=1, learning_rate=0.0001, kl_tolerance=0.5, is_training=False, reuse=False, gpu_mode=False):\n",
    "        self.z_size = z_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "        self.is_training = is_training\n",
    "        self.reuse = reuse\n",
    "        with tf.variable_scope('conv_vae', reuse=self.reuse):\n",
    "            if not gpu_mode:\n",
    "                with tf.device('/cpu:0'):\n",
    "                    tf.logging.info('Model using cpu.')\n",
    "                    self._build_graph()\n",
    "            else:\n",
    "                tf.logging.info('Model using gpu.')\n",
    "                self._build_graph()\n",
    "        self._init_session()\n",
    "    \n",
    "    # Making a method that creates the VAE model architecture itself\n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])\n",
    "            # Building the Encoder part of the VAE\n",
    "            h = tf.layers.conv2d(self.x, 32, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv1\")\n",
    "            h = tf.layers.conv2d(h, 64, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv2\")\n",
    "            h = tf.layers.conv2d(h, 128, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv3\")\n",
    "            h = tf.layers.conv2d(h, 256, 4, strides=2, activation=tf.nn.relu, name=\"enc_conv4\")\n",
    "            h = tf.reshape(h, [-1, 2*2*256])\n",
    "            # Building the \"V\" part of the VAE\n",
    "            self.mu = tf.layers.dense(h, self.z_size, name=\"enc_fc_mu\")\n",
    "            self.logvar = tf.layers.dense(h, self.z_size, name=\"enc_fc_log_var\")\n",
    "            self.sigma = tf.exp(self.logvar / 2.0)\n",
    "            self.epsilon = tf.random_normal([self.batch_size, self.z_size])\n",
    "            self.z = self.mu + self.sigma * self.epsilon\n",
    "            # Building the Decoder part of the VAE\n",
    "            h = tf.layers.dense(self.z, 1024, name=\"dec_fc\")\n",
    "            h = tf.reshape(h, [-1, 1, 1, 1024])\n",
    "            h = tf.layers.conv2d_transpose(h, 128, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv1\")\n",
    "            h = tf.layers.conv2d_transpose(h, 64, 5, strides=2, activation=tf.nn.relu, name=\"dec_deconv2\")\n",
    "            h = tf.layers.conv2d_transpose(h, 32, 6, strides=2, activation=tf.nn.relu, name=\"dec_deconv3\")\n",
    "            self.y = tf.layers.conv2d_transpose(h, 3, 6, strides=2, activation=tf.nn.sigmoid, name=\"dec_deconv4\")\n",
    "            # Implementing the training operations\n",
    "            if self.is_training:\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                self.r_loss = tf.reduce_sum(tf.square(self.x - self.y), reduction_indices = [1,2,3])\n",
    "                self.r_loss = tf.reduce_mean(self.r_loss)\n",
    "                self.kl_loss = - 0.5 * tf.reduce_sum((1 + self.logvar - tf.square(self.mu) - tf.exp(self.logvar)), reduction_indices = 1)\n",
    "                self.kl_loss = tf.maximum(self.kl_loss, self.kl_tolerance * self.z_size)\n",
    "                self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "                self.loss = self.r_loss + self.kl_loss\n",
    "                self.lr = tf.Variable(self.learning_rate, trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "                grads = self.optimizer.compute_gradients(self.loss)\n",
    "                self.train_op = self.optimizer.apply_gradients(grads, global_step=self.global_step, name='train_step')\n",
    "            self.init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Building the CNN-VAE model with Keras\n",
    " \n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    " \n",
    "# Setting the input dimensions (size of frames)\n",
    "INPUT_DIM = (64,64,3)\n",
    " \n",
    "# Setting the number of convolutional filters, kernel sizes, strides and activations per layer\n",
    "CONV_FILTERS = [32,64,64,128]\n",
    "CONV_KERNEL_SIZES = [4,4,4,4]\n",
    "CONV_STRIDES = [2,2,2,2]\n",
    "CONV_ACTIVATIONS = ['relu','relu','relu','relu']\n",
    " \n",
    "# Setting the dense layer size\n",
    "DENSE_SIZE = 1024\n",
    " \n",
    "# Setting the layer parameters for the decoder part of the VAE\n",
    "CONV_T_FILTERS = [64,64,32,3]\n",
    "CONV_T_KERNEL_SIZES = [5,5,6,6]\n",
    "CONV_T_STRIDES = [2,2,2,2]\n",
    "CONV_T_ACTIVATIONS = ['relu','relu','relu','sigmoid']\n",
    " \n",
    "# Setting the dimensions of the latent vectors\n",
    "Z_DIM = 32\n",
    " \n",
    "# Setting the number of epochs and batch size\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    " \n",
    "# Making a function that creates centralized latent vectors for the VAE\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Z_DIM), mean=0.,stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    " \n",
    "# Building the CNN-VAE model within a class\n",
    " \n",
    "class ConvVAE():\n",
    " \n",
    "    # Initializing all the parameters and variables of the ConvVAE class\n",
    "    def __init__(self):\n",
    "        self.models = self._build()\n",
    "        self.model = self.models[0]\n",
    "        self.encoder = self.models[1]\n",
    "        self.decoder = self.models[2]\n",
    "        self.input_dim = INPUT_DIM\n",
    "        self.z_dim = Z_DIM\n",
    " \n",
    "    # Building the model\n",
    "    def _build(self):\n",
    "        # Creating the model and the encoder inputs\n",
    "        vae_x = Input(shape=INPUT_DIM)\n",
    "        # Creating the first convolutional layer of the Encoder\n",
    "        vae_c1 = Conv2D(filters = CONV_FILTERS[0], kernel_size = CONV_KERNEL_SIZES[0], strides = CONV_STRIDES[0], activation=CONV_ACTIVATIONS[0])(vae_x)\n",
    "        # Creating the second convolutional layer of the Encoder\n",
    "        vae_c2 = Conv2D(filters = CONV_FILTERS[1], kernel_size = CONV_KERNEL_SIZES[1], strides = CONV_STRIDES[1], activation=CONV_ACTIVATIONS[0])(vae_c1)\n",
    "        # Creating the third convolutional layer of the Encoder\n",
    "        vae_c3= Conv2D(filters = CONV_FILTERS[2], kernel_size = CONV_KERNEL_SIZES[2], strides = CONV_STRIDES[2], activation=CONV_ACTIVATIONS[0])(vae_c2)\n",
    "        # Creating the fourth convolutional layer of the Encoder\n",
    "        vae_c4= Conv2D(filters = CONV_FILTERS[3], kernel_size = CONV_KERNEL_SIZES[3], strides = CONV_STRIDES[3], activation=CONV_ACTIVATIONS[0])(vae_c3)\n",
    "        # Flattening the last convolutional layer so we can input it in the dense layers\n",
    "        vae_z_in = Flatten()(vae_c4)\n",
    "        # Using two separate files to calculate z_mean and z_log\n",
    "        vae_z_mean = Dense(Z_DIM)(vae_z_in)\n",
    "        vae_z_log_var = Dense(Z_DIM)(vae_z_in)\n",
    "        # Using the Lambda Keras class around the sampling function we created above\n",
    "        vae_z = Lambda(sampling)([vae_z_mean, vae_z_log_var])\n",
    "        # Getting the inputs of the decoder part\n",
    "        vae_z_input = Input(shape=(Z_DIM,))\n",
    "        # Instantiating these layers separately so as to reuse them later\n",
    "        vae_dense = Dense(1024)\n",
    "        vae_dense_model = vae_dense(vae_z)\n",
    "        # Reshaping the dense layer to 4 dimentions, so we can put it through the transposed convolution\n",
    "        vae_z_out = Reshape((1,1,DENSE_SIZE))\n",
    "        # Getting the output from this last layer\n",
    "        vae_z_out_model = vae_z_out(vae_dense_model)\n",
    "        # Defining the first transposed convolutional layer\n",
    "        vae_d1 = Conv2DTranspose(filters = CONV_T_FILTERS[0], kernel_size = CONV_T_KERNEL_SIZES[0] , strides = CONV_T_STRIDES[0], activation=CONV_T_ACTIVATIONS[0])\n",
    "        # Creating the first decoder layer\n",
    "        vae_d1_model = vae_d1(vae_z_out_model)\n",
    "        # Defining the second transposed convolutional layer\n",
    "        vae_d2 = Conv2DTranspose(filters = CONV_T_FILTERS[1], kernel_size = CONV_T_KERNEL_SIZES[1] , strides = CONV_T_STRIDES[1], activation=CONV_T_ACTIVATIONS[1])\n",
    "        # Creating the second decoder layer\n",
    "        vae_d2_model = vae_d2(vae_d1_model)\n",
    "        # Defining the third convolutional layer\n",
    "        vae_d3 = Conv2DTranspose(filters = CONV_T_FILTERS[2], kernel_size = CONV_T_KERNEL_SIZES[2] , strides = CONV_T_STRIDES[2], activation=CONV_T_ACTIVATIONS[2])\n",
    "        # Creating the third decoder layer\n",
    "        vae_d3_model = vae_d3(vae_d2_model)\n",
    "        # Defining the fourth convolutional layer\n",
    "        vae_d4 = Conv2DTranspose(filters = CONV_T_FILTERS[3], kernel_size = CONV_T_KERNEL_SIZES[3] , strides = CONV_T_STRIDES[3], activation=CONV_T_ACTIVATIONS[3])\n",
    "        # Creating the fourth decoder layer\n",
    "        vae_d4_model = vae_d4(vae_d3_model)\n",
    "        # Getting the latent vector output of the decoder\n",
    "        vae_dense_decoder = vae_dense(vae_z_input)\n",
    "        vae_z_out_decoder = vae_z_out(vae_dense_decoder)\n",
    "        vae_d1_decoder = vae_d1(vae_z_out_decoder)\n",
    "        vae_d2_decoder = vae_d2(vae_d1_decoder)\n",
    "        vae_d3_decoder = vae_d3(vae_d2_decoder)\n",
    "        vae_d4_decoder = vae_d4(vae_d3_decoder)\n",
    "        # Defining the end-to-end VAE Model, composed of both the encoder and the decoder\n",
    "        vae = Model(vae_x, vae_d4_model)\n",
    "        vae_encoder = Model(vae_x, vae_z)\n",
    "        vae_decoder = Model(vae_z_input, vae_d4_decoder)\n",
    "        # Implementing the training operations\n",
    "        # Defining the MSE loss\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            y_true_flat = K.flatten(y_true)\n",
    "            y_pred_flat = K.flatten(y_pred)\n",
    "            return 10 * K.mean(K.square(y_true_flat - y_pred_flat), axis = -1)\n",
    "        # Defining the KL divergence loss\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            return - 0.5 * K.mean(1 + vae_z_log_var - K.square(vae_z_mean) - K.exp(vae_z_log_var), axis = -1)\n",
    "        # Defining the total VAE loss, summing the MSE and KL losses\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            return vae_r_loss(y_true, y_pred) + vae_kl_loss(y_true, y_pred)\n",
    "        # Compiling the whole model with the RMSProp optimizer, the vae loss and custom metrics\n",
    "        vae.compile(optimizer='rmsprop', loss = vae_loss,  metrics = [vae_r_loss, vae_kl_loss])\n",
    "        return (vae, vae_encoder, vae_decoder)\n",
    " \n",
    "    # Loading the model\n",
    "    def set_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    " \n",
    "    # Creating early stopping callbacks to prevent overfitting\n",
    "    def train(self, data, validation_split = 0.2):\n",
    "        earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "        callbacks_list = [earlystop]\n",
    "        self.model.fit(data, data,\n",
    "                       shuffle=True,\n",
    "                       epochs=EPOCHS,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       validation_split=validation_split,\n",
    "                       callbacks=callbacks_list)\n",
    "        self.model.save_weights('vae/weights.h5')\n",
    " \n",
    "    # Saving the model\n",
    "    def save_weights(self, filepath):\n",
    "        self.model.save_weights(filepath)\n",
    "    \n",
    "    # Generating data for the MDN-RNN\n",
    "    def generate_rnn_data(self, obs_data, action_data):\n",
    "        rnn_input = []\n",
    "        rnn_output = []\n",
    "        for i, j in zip(obs_data, action_data):    \n",
    "            rnn_z_input = self.encoder.predict(np.array(i))\n",
    "            conc = [np.concatenate([x,y]) for x, y in zip(rnn_z_input, j.reshape(-1, 1))]\n",
    "            rnn_input.append(conc[:-1])\n",
    "            rnn_output.append(np.array(rnn_z_input[1:]))\n",
    "        rnn_input = np.array(rnn_input)\n",
    "        rnn_output = np.array(rnn_output)\n",
    "        print(\"Rnn inputs size: {}\".format(rnn_input.shape), \" Rnn outputs size: {}\".format(rnn_output.shape))\n",
    "        return (rnn_input, rnn_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
